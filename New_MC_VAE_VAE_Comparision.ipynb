{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New_MC VAE - VAE - Comparision.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdT9BTrmT3ro",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNUpL6YT32X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.layers import Lambda, Input, Dense, Dropout\n",
        "\n",
        "GLOBAL_SEED = 1\n",
        "LOCAL_SEED = 42\n",
        "\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjJAxu80AFW6"
      },
      "source": [
        "# Access Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaXts2Ycg-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define PATH to file\n",
        "path = 'gdrive/My Drive/Generators/DataSets/Selected/breast-cancer-wisconsin/wdbc.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/balance-scale/balance-scale.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/pima-indians-diabetes/pima-indians-diabetes.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/tic-tac-toe/tic-tac-toe.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/annealing/anneal.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/breast-cancer/breast-cancer.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/cylinder-bands/bands.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/credit-screening/crx.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/statlog/australian/australian.dat'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/statlog/german/german.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/statlog/german/german.data-numeric'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/spectrometer/lrs.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/soybean/soybean-large.data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8zWTy_ePGg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intermediate_dim = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VTGWwfObeoT",
        "colab_type": "text"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfcE15dwhyaL",
        "colab_type": "code",
        "outputId": "bd0d3c50-e841-4ecd-e0b6-04393556a1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "import pandas as pd\n",
        "na_values = {'?', ' '}\n",
        "df = pd.read_csv(path,\n",
        "                 sep=',',\n",
        "                 header=None,\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "print('Origin dataset:')                 \n",
        "print(df.head())\n",
        "# Drop N/A \n",
        "df.dropna(axis=0, how='any', inplace=True)\n",
        "df.replace('U',np.NaN, inplace=True)\n",
        "# print(df.head())\n",
        "\n",
        "# For Breast cancer \n",
        "# Drop ID column\n",
        "df.drop([0], axis=1, inplace=True)\n",
        "\n",
        "# For German; 20 attribute; the last columns\n",
        "# df.drop([20], axis=1, inplace=True)\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "print(\"After remove\")\n",
        "print(df.head())\n",
        "\n",
        "col_names = list(df)\n",
        "new_names = {}\n",
        "for i, name in enumerate(col_names):\n",
        "    new_names[name] = 'X' + str(i)\n",
        "df.rename(columns=new_names, inplace=True)\n",
        "\n",
        "\n",
        "# For soybean\n",
        "# colnums = len(df.columns)\n",
        "# for i in df.columns:\n",
        "#     df[i] = df[i].astype('category')\n",
        "\n",
        "# For Pima diabetes\n",
        "# df['X9'] = df['X9'].astype('category')\n",
        "# df['X8'] = df['X8'].astype('category')\n",
        "\n",
        "# For Breast Cancer wincosin\n",
        "# df['X9'] = df['X9'].astype('category')\n",
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Origin dataset:\n",
            "         0  1      2      3       4   ...      27      28      29      30       31\n",
            "0    842302  M  17.99  10.38  122.80  ...  0.6656  0.7119  0.2654  0.4601  0.11890\n",
            "1    842517  M  20.57  17.77  132.90  ...  0.1866  0.2416  0.1860  0.2750  0.08902\n",
            "2  84300903  M  19.69  21.25  130.00  ...  0.4245  0.4504  0.2430  0.3613  0.08758\n",
            "3  84348301  M  11.42  20.38   77.58  ...  0.8663  0.6869  0.2575  0.6638  0.17300\n",
            "4  84358402  M  20.29  14.34  135.10  ...  0.2050  0.4000  0.1625  0.2364  0.07678\n",
            "\n",
            "[5 rows x 32 columns]\n",
            "After remove\n",
            "  1      2      3       4       5   ...      27      28      29      30       31\n",
            "0  M  17.99  10.38  122.80  1001.0  ...  0.6656  0.7119  0.2654  0.4601  0.11890\n",
            "1  M  20.57  17.77  132.90  1326.0  ...  0.1866  0.2416  0.1860  0.2750  0.08902\n",
            "2  M  19.69  21.25  130.00  1203.0  ...  0.4245  0.4504  0.2430  0.3613  0.08758\n",
            "3  M  11.42  20.38   77.58   386.1  ...  0.8663  0.6869  0.2575  0.6638  0.17300\n",
            "4  M  20.29  14.34  135.10  1297.0  ...  0.2050  0.4000  0.1625  0.2364  0.07678\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "  X0     X1      X10     X11     X12  ...       X5       X6      X7       X8      X9\n",
            "0  M  17.99  0.07871  1.0950  0.9053  ...  0.11840  0.27760  0.3001  0.14710  0.2419\n",
            "1  M  20.57  0.05667  0.5435  0.7339  ...  0.08474  0.07864  0.0869  0.07017  0.1812\n",
            "2  M  19.69  0.05999  0.7456  0.7869  ...  0.10960  0.15990  0.1974  0.12790  0.2069\n",
            "3  M  11.42  0.09744  0.4956  1.1560  ...  0.14250  0.28390  0.2414  0.10520  0.2597\n",
            "4  M  20.29  0.05883  0.7572  0.7813  ...  0.10030  0.13280  0.1980  0.10430  0.1809\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obi16ACOX7mf",
        "colab_type": "code",
        "outputId": "f372097d-2204-4be1-986d-f0eb065265da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "latent_dim = len(df.columns)//2\n",
        "if latent_dim < 2:\n",
        "    latent_dim = 2\n",
        "print('Latent Dim = ', latent_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latent Dim =  15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Yk-VnN1mlc",
        "colab_type": "text"
      },
      "source": [
        "# Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpGxC2v2CDi1",
        "colab_type": "code",
        "outputId": "e76da31b-82a8-4e51-ca05-0de99d4b3f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "vals = np.copy(df.values)\n",
        "total_nums = len(vals)\n",
        "\n",
        "df_train, df_validation = train_test_split(df, test_size=0.5, \n",
        "                                           random_state=LOCAL_SEED, \n",
        "                                           shuffle=True)\n",
        "# Write the test dataset\n",
        "df_validation = df_validation.reindex(sorted(df_validation.columns), axis=1)\n",
        "df_validation.to_csv(path + '_For_Test.csv', index=False)\n",
        "print(df_validation.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    X0     X1      X10     X11  ...      X6       X7       X8      X9\n",
            "204  B  12.47  0.06373  0.3961  ...  0.1058  0.08005  0.03821  0.1925\n",
            "70   M  18.94  0.05461  0.7888  ...  0.1029  0.10800  0.07951  0.1582\n",
            "131  M  15.46  0.05796  0.4743  ...  0.1223  0.14660  0.08087  0.1931\n",
            "431  B  12.40  0.07102  0.1767  ...  0.1316  0.07741  0.02799  0.1811\n",
            "540  B  11.54  0.06782  0.2784  ...  0.1120  0.06737  0.02594  0.1818\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptmiXDEjbhmp",
        "colab_type": "text"
      },
      "source": [
        "# Recognize categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "73935d4a-3431-4cf1-8552-35fb3f06ebc6",
        "id": "4nd-yR5cvyge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df = df_train.copy(deep=True)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    X0     X1      X10     X11  ...       X6       X7        X8      X9\n",
            "423  B  13.66  0.06181  0.2244  ...  0.11470  0.09657  0.048120  0.1848\n",
            "546  B  10.32  0.06201  0.2104  ...  0.04994  0.01012  0.005495  0.1885\n",
            "119  M  17.95  0.05025  0.5506  ...  0.06722  0.07293  0.055960  0.2129\n",
            "386  B  12.21  0.06154  0.2666  ...  0.07823  0.06839  0.025340  0.1646\n",
            "367  B  12.21  0.05916  0.2527  ...  0.07175  0.04392  0.020270  0.1695\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfL6yO7HZR7E",
        "colab_type": "code",
        "outputId": "4956bd0a-215a-4b04-bb43-1ea6ff7fa81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# For breast cancer\n",
        "# df['X9'] = df['X9'].astype('category')\n",
        "# For Pima Diabetes\n",
        "# df['X8'] = df['X8'].astype('category')\n",
        "colnums = len(df.columns)\n",
        "for i in df.columns:\n",
        "    try:\n",
        "        if df[i].dtype.name == 'object':\n",
        "            df[i] = df[i].astype('category')\n",
        "        else:\n",
        "            df[i].astype('float32')\n",
        "    except:\n",
        "        continue\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    X0     X1      X10     X11  ...       X6       X7        X8      X9\n",
            "423  B  13.66  0.06181  0.2244  ...  0.11470  0.09657  0.048120  0.1848\n",
            "546  B  10.32  0.06201  0.2104  ...  0.04994  0.01012  0.005495  0.1885\n",
            "119  M  17.95  0.05025  0.5506  ...  0.06722  0.07293  0.055960  0.2129\n",
            "386  B  12.21  0.06154  0.2666  ...  0.07823  0.06839  0.025340  0.1646\n",
            "367  B  12.21  0.05916  0.2527  ...  0.07175  0.04392  0.020270  0.1695\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "               X1         X10         X11  ...          X7          X8          X9\n",
            "count  284.000000  284.000000  284.000000  ...  284.000000  284.000000  284.000000\n",
            "mean    14.382239    0.062422    0.414679  ...    0.091528    0.050420    0.181324\n",
            "std      3.658528    0.007022    0.311197  ...    0.080519    0.038811    0.026450\n",
            "min      8.219000    0.050250    0.111500  ...    0.000000    0.000000    0.116700\n",
            "25%     11.790000    0.057260    0.238775  ...    0.029635    0.020698    0.163075\n",
            "50%     13.435000    0.061270    0.333550  ...    0.066885    0.037210    0.180000\n",
            "75%     16.387500    0.065750    0.470000  ...    0.133825    0.077363    0.196450\n",
            "max     28.110000    0.095750    2.873000  ...    0.426800    0.201200    0.267800\n",
            "\n",
            "[8 rows x 30 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-52cnSrZHGm",
        "colab_type": "code",
        "outputId": "5f9c9397-9283-4f79-ac98-ec420d6bc7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# df['X9'] = df['X9'].astype('category')\n",
        "# df['X8'] = df['X8'].astype('category')\n",
        "categorical = df.select_dtypes(['category']).columns\n",
        "print(categorical)\n",
        "for f in categorical:\n",
        "    dummies = pd.get_dummies(df[f], prefix = f, prefix_sep = '_')\n",
        "    df = pd.concat([df, dummies], axis = 1)\n",
        "    \n",
        "# drop original categorical features\n",
        "df.drop(categorical, axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['X0'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9M4zr70tl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(path + 'For_training.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnJXo9LfUGaJ",
        "colab_type": "text"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYeYLHK2fpXX",
        "colab_type": "text"
      },
      "source": [
        "## Split train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmIPmd5wjAM7",
        "colab_type": "code",
        "outputId": "bf86ba5f-17e6-40c7-b53c-5f7788811369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "df = pd.read_csv(path + 'For_training.csv')\n",
        "vae_train = np.copy(df.values)\n",
        "vae_train.astype('float32')\n",
        "scaler = MinMaxScaler()\n",
        "print(np.amax(vae_train[:, 2]))\n",
        "\n",
        "vae_train = scaler.fit_transform(vae_train)\n",
        "x_train, x_test = train_test_split(vae_train, test_size=0.5,\n",
        "                                   random_state=LOCAL_SEED,\n",
        "                                   shuffle=True)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(np.amax(x_train))\n",
        "print(np.amax(x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.873\n",
            "(142, 32)\n",
            "(142, 32)\n",
            "1.0000000000000002\n",
            "1.0000000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNf-3EMl2usm",
        "colab_type": "code",
        "outputId": "de204767-a214-4247-a06a-46580bfdf6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "original_dim = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 32)\n",
            "(142, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2vq5zxdfvq7",
        "colab_type": "text"
      },
      "source": [
        "## Define VAE class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-JqW7Jpuyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "class VAE:\n",
        "    def __init__(self, input_shape=(original_dim,), \n",
        "                 intermediate_dim=128, latent_dim=2, summary=False):\n",
        "        \n",
        "        self._build_model(input_shape,\n",
        "                         intermediate_dim, \n",
        "                          latent_dim, summary)\n",
        "    \n",
        "    def _build_model(self, input_shape, intermediate_dim, latent_dim,\n",
        "                    summary=False):\n",
        "        inputs = Input(shape=input_shape, name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(intermediate_dim, activation='relu')(x)\n",
        "        x = Dense(intermediate_dim//2, activation='relu')(x)\n",
        "        \n",
        "        z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        z = Lambda(self.sampling, output_shape=(latent_dim,), \n",
        "                   name='z')([z_mean, z_log_var])\n",
        "\n",
        "        self.encoder = Model(inputs, [z_mean, z_log_var, z], \n",
        "                        name='encoder')\n",
        "        \n",
        "        latent_inputs = Input(shape=(latent_dim,), \n",
        "                              name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(intermediate_dim//2, activation='relu')(x)\n",
        "        x = Dense(intermediate_dim, activation='relu')(x)\n",
        "        outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs)[2])\n",
        "        self.vae = Model(inputs, outputs, name='vae_mlp')\n",
        "        \n",
        "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "        reconstruction_loss *= original_dim\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        \n",
        "        vae_loss = K.mean(reconstruction_loss + kl_loss)\t\n",
        "        \n",
        "        self.vae.add_loss(vae_loss)\n",
        "        self.vae.compile(optimizer='adam')\n",
        "        if summary: \n",
        "            print(self.vae.summary())\n",
        "        \n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "        \n",
        "    def fit(self, x_train, x_test, epochs=100, batch_size=100,\n",
        "           verbose=1):\n",
        "        self.vae.fit(x_train, \n",
        "            shuffle=True,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=verbose,\n",
        "            validation_data=(x_test, None))\n",
        "    \n",
        "    def encoder_predict(self, x_test, batch_size=100):\n",
        "        return self.encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    \n",
        "    def generate(self, latent_val, batch_size=100):\n",
        "        return self.decoder.predict(latent_val)\n",
        "    \n",
        "    def predict(self, x_test, batch_size=1):\n",
        "        prediction = self.vae.predict(x_test)\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXhRjx02UYZN",
        "colab_type": "text"
      },
      "source": [
        "## Training VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd02caRJWGG8",
        "colab_type": "text"
      },
      "source": [
        "Just let the last value to test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIQUNfHLkey_",
        "colab_type": "code",
        "outputId": "f5477d68-a631-4521-e59f-be43364e2836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(np.amax(x_train))\n",
        "print(np.amin(x_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 32)\n",
            "1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBbIF17p8iY2",
        "colab_type": "code",
        "outputId": "b71b7bc2-6353-4dcf-c9c0-c3a2b81bf6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5672
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "vae = VAE(intermediate_dim=intermediate_dim, latent_dim=latent_dim)\n",
        "vae.fit(x_train, x_test, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 07:22:30.286930 140529874950016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0614 07:22:30.299089 140529874950016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0614 07:22:30.300383 140529874950016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0614 07:22:30.351479 140529874950016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "W0614 07:22:30.425449 140529874950016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0614 07:22:30.431617 140529874950016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0614 07:22:30.457518 140529874950016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 142 samples, validate on 142 samples\n",
            "Epoch 1/150\n",
            "142/142 [==============================] - 4s 31ms/step - loss: 21.8389 - val_loss: 20.3945\n",
            "Epoch 2/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 20.3213 - val_loss: 19.1447\n",
            "Epoch 3/150\n",
            "142/142 [==============================] - 0s 97us/step - loss: 19.1193 - val_loss: 18.2936\n",
            "Epoch 4/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 18.3954 - val_loss: 17.8491\n",
            "Epoch 5/150\n",
            "142/142 [==============================] - 0s 105us/step - loss: 18.0656 - val_loss: 17.7671\n",
            "Epoch 6/150\n",
            "142/142 [==============================] - 0s 113us/step - loss: 18.2215 - val_loss: 17.9543\n",
            "Epoch 7/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 18.2027 - val_loss: 17.8183\n",
            "Epoch 8/150\n",
            "142/142 [==============================] - 0s 102us/step - loss: 18.0965 - val_loss: 17.8638\n",
            "Epoch 9/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 18.0864 - val_loss: 17.7290\n",
            "Epoch 10/150\n",
            "142/142 [==============================] - 0s 106us/step - loss: 18.0484 - val_loss: 17.7597\n",
            "Epoch 11/150\n",
            "142/142 [==============================] - 0s 115us/step - loss: 18.0635 - val_loss: 17.6327\n",
            "Epoch 12/150\n",
            "142/142 [==============================] - 0s 129us/step - loss: 17.8760 - val_loss: 17.7644\n",
            "Epoch 13/150\n",
            "142/142 [==============================] - 0s 150us/step - loss: 17.9084 - val_loss: 17.5588\n",
            "Epoch 14/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 17.7956 - val_loss: 17.5258\n",
            "Epoch 15/150\n",
            "142/142 [==============================] - 0s 113us/step - loss: 17.6583 - val_loss: 17.6063\n",
            "Epoch 16/150\n",
            "142/142 [==============================] - 0s 105us/step - loss: 17.8810 - val_loss: 17.6278\n",
            "Epoch 17/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 17.8702 - val_loss: 17.5237\n",
            "Epoch 18/150\n",
            "142/142 [==============================] - 0s 105us/step - loss: 17.7622 - val_loss: 17.5034\n",
            "Epoch 19/150\n",
            "142/142 [==============================] - 0s 103us/step - loss: 17.6958 - val_loss: 17.3478\n",
            "Epoch 20/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 17.6680 - val_loss: 17.4536\n",
            "Epoch 21/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 17.7702 - val_loss: 17.4214\n",
            "Epoch 22/150\n",
            "142/142 [==============================] - 0s 112us/step - loss: 17.6152 - val_loss: 17.4040\n",
            "Epoch 23/150\n",
            "142/142 [==============================] - 0s 118us/step - loss: 17.7607 - val_loss: 17.2833\n",
            "Epoch 24/150\n",
            "142/142 [==============================] - 0s 107us/step - loss: 17.6401 - val_loss: 17.3555\n",
            "Epoch 25/150\n",
            "142/142 [==============================] - 0s 106us/step - loss: 17.4401 - val_loss: 17.2551\n",
            "Epoch 26/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 17.5468 - val_loss: 17.2953\n",
            "Epoch 27/150\n",
            "142/142 [==============================] - 0s 96us/step - loss: 17.5005 - val_loss: 17.4286\n",
            "Epoch 28/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 17.5733 - val_loss: 17.1062\n",
            "Epoch 29/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 17.3343 - val_loss: 17.2505\n",
            "Epoch 30/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 17.3710 - val_loss: 16.9827\n",
            "Epoch 31/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 17.3251 - val_loss: 17.4657\n",
            "Epoch 32/150\n",
            "142/142 [==============================] - 0s 102us/step - loss: 17.4401 - val_loss: 17.2690\n",
            "Epoch 33/150\n",
            "142/142 [==============================] - 0s 107us/step - loss: 17.4134 - val_loss: 17.2089\n",
            "Epoch 34/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 17.3466 - val_loss: 17.3681\n",
            "Epoch 35/150\n",
            "142/142 [==============================] - 0s 172us/step - loss: 17.4097 - val_loss: 17.1882\n",
            "Epoch 36/150\n",
            "142/142 [==============================] - 0s 179us/step - loss: 17.3275 - val_loss: 17.2636\n",
            "Epoch 37/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 17.2337 - val_loss: 17.0779\n",
            "Epoch 38/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 17.3910 - val_loss: 17.2716\n",
            "Epoch 39/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 17.3988 - val_loss: 17.3645\n",
            "Epoch 40/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 17.3492 - val_loss: 17.2579\n",
            "Epoch 41/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 17.4281 - val_loss: 17.2946\n",
            "Epoch 42/150\n",
            "142/142 [==============================] - 0s 112us/step - loss: 17.2864 - val_loss: 17.7272\n",
            "Epoch 43/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 17.2364 - val_loss: 17.2845\n",
            "Epoch 44/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 17.0655 - val_loss: 17.2053\n",
            "Epoch 45/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 17.2673 - val_loss: 17.2120\n",
            "Epoch 46/150\n",
            "142/142 [==============================] - 0s 124us/step - loss: 17.5912 - val_loss: 17.2322\n",
            "Epoch 47/150\n",
            "142/142 [==============================] - 0s 135us/step - loss: 17.3361 - val_loss: 17.4429\n",
            "Epoch 48/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 16.9791 - val_loss: 17.2956\n",
            "Epoch 49/150\n",
            "142/142 [==============================] - 0s 132us/step - loss: 17.2867 - val_loss: 17.1431\n",
            "Epoch 50/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 17.3996 - val_loss: 17.2402\n",
            "Epoch 51/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 17.2408 - val_loss: 17.1516\n",
            "Epoch 52/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 17.3296 - val_loss: 17.2154\n",
            "Epoch 53/150\n",
            "142/142 [==============================] - 0s 106us/step - loss: 17.1467 - val_loss: 17.1739\n",
            "Epoch 54/150\n",
            "142/142 [==============================] - 0s 107us/step - loss: 17.1386 - val_loss: 17.1940\n",
            "Epoch 55/150\n",
            "142/142 [==============================] - 0s 125us/step - loss: 17.2209 - val_loss: 17.1198\n",
            "Epoch 56/150\n",
            "142/142 [==============================] - 0s 107us/step - loss: 17.1336 - val_loss: 17.1843\n",
            "Epoch 57/150\n",
            "142/142 [==============================] - 0s 125us/step - loss: 17.3252 - val_loss: 17.1225\n",
            "Epoch 58/150\n",
            "142/142 [==============================] - 0s 155us/step - loss: 17.5057 - val_loss: 17.0021\n",
            "Epoch 59/150\n",
            "142/142 [==============================] - 0s 121us/step - loss: 17.1300 - val_loss: 17.2432\n",
            "Epoch 60/150\n",
            "142/142 [==============================] - 0s 115us/step - loss: 17.1247 - val_loss: 17.1716\n",
            "Epoch 61/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 17.2269 - val_loss: 17.2875\n",
            "Epoch 62/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 17.4823 - val_loss: 17.6328\n",
            "Epoch 63/150\n",
            "142/142 [==============================] - 0s 118us/step - loss: 17.2085 - val_loss: 17.3695\n",
            "Epoch 64/150\n",
            "142/142 [==============================] - 0s 103us/step - loss: 17.1855 - val_loss: 17.1461\n",
            "Epoch 65/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 17.1600 - val_loss: 17.3467\n",
            "Epoch 66/150\n",
            "142/142 [==============================] - 0s 134us/step - loss: 17.2061 - val_loss: 17.2710\n",
            "Epoch 67/150\n",
            "142/142 [==============================] - 0s 151us/step - loss: 17.2657 - val_loss: 17.1106\n",
            "Epoch 68/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 17.2313 - val_loss: 17.1420\n",
            "Epoch 69/150\n",
            "142/142 [==============================] - 0s 112us/step - loss: 17.1547 - val_loss: 17.2284\n",
            "Epoch 70/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 17.3925 - val_loss: 17.1249\n",
            "Epoch 71/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 17.3875 - val_loss: 17.1351\n",
            "Epoch 72/150\n",
            "142/142 [==============================] - 0s 116us/step - loss: 17.2092 - val_loss: 17.0705\n",
            "Epoch 73/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 17.1324 - val_loss: 16.9645\n",
            "Epoch 74/150\n",
            "142/142 [==============================] - 0s 121us/step - loss: 17.1259 - val_loss: 16.9994\n",
            "Epoch 75/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 17.2163 - val_loss: 17.1728\n",
            "Epoch 76/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 17.1527 - val_loss: 16.8820\n",
            "Epoch 77/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 17.2075 - val_loss: 17.0755\n",
            "Epoch 78/150\n",
            "142/142 [==============================] - 0s 129us/step - loss: 17.0906 - val_loss: 16.9350\n",
            "Epoch 79/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 17.1256 - val_loss: 17.1403\n",
            "Epoch 80/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 16.9870 - val_loss: 17.0507\n",
            "Epoch 81/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 17.1220 - val_loss: 17.0189\n",
            "Epoch 82/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 17.2052 - val_loss: 17.0690\n",
            "Epoch 83/150\n",
            "142/142 [==============================] - 0s 116us/step - loss: 16.9680 - val_loss: 16.9675\n",
            "Epoch 84/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 17.1117 - val_loss: 17.0647\n",
            "Epoch 85/150\n",
            "142/142 [==============================] - 0s 115us/step - loss: 17.1796 - val_loss: 16.9888\n",
            "Epoch 86/150\n",
            "142/142 [==============================] - 0s 113us/step - loss: 17.0142 - val_loss: 16.9578\n",
            "Epoch 87/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 17.0869 - val_loss: 17.0345\n",
            "Epoch 88/150\n",
            "142/142 [==============================] - 0s 124us/step - loss: 17.0783 - val_loss: 16.9848\n",
            "Epoch 89/150\n",
            "142/142 [==============================] - 0s 118us/step - loss: 17.0056 - val_loss: 16.9988\n",
            "Epoch 90/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 17.1048 - val_loss: 17.0326\n",
            "Epoch 91/150\n",
            "142/142 [==============================] - 0s 148us/step - loss: 17.0992 - val_loss: 16.8480\n",
            "Epoch 92/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 17.0465 - val_loss: 16.9960\n",
            "Epoch 93/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 17.1247 - val_loss: 16.9968\n",
            "Epoch 94/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 17.0153 - val_loss: 16.9048\n",
            "Epoch 95/150\n",
            "142/142 [==============================] - 0s 98us/step - loss: 16.9909 - val_loss: 16.9389\n",
            "Epoch 96/150\n",
            "142/142 [==============================] - 0s 113us/step - loss: 16.9323 - val_loss: 16.8495\n",
            "Epoch 97/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 16.8546 - val_loss: 16.8759\n",
            "Epoch 98/150\n",
            "142/142 [==============================] - 0s 168us/step - loss: 16.9833 - val_loss: 16.7684\n",
            "Epoch 99/150\n",
            "142/142 [==============================] - 0s 106us/step - loss: 16.8208 - val_loss: 17.0025\n",
            "Epoch 100/150\n",
            "142/142 [==============================] - 0s 103us/step - loss: 16.8577 - val_loss: 16.9338\n",
            "Epoch 101/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 16.9287 - val_loss: 16.8601\n",
            "Epoch 102/150\n",
            "142/142 [==============================] - 0s 121us/step - loss: 16.7578 - val_loss: 16.9927\n",
            "Epoch 103/150\n",
            "142/142 [==============================] - 0s 131us/step - loss: 16.7873 - val_loss: 16.8792\n",
            "Epoch 104/150\n",
            "142/142 [==============================] - 0s 116us/step - loss: 16.6753 - val_loss: 16.9889\n",
            "Epoch 105/150\n",
            "142/142 [==============================] - 0s 96us/step - loss: 16.9338 - val_loss: 16.9018\n",
            "Epoch 106/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 16.8154 - val_loss: 16.7236\n",
            "Epoch 107/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 16.9519 - val_loss: 16.9288\n",
            "Epoch 108/150\n",
            "142/142 [==============================] - 0s 136us/step - loss: 16.9144 - val_loss: 16.9513\n",
            "Epoch 109/150\n",
            "142/142 [==============================] - 0s 106us/step - loss: 16.8962 - val_loss: 16.8584\n",
            "Epoch 110/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 16.8385 - val_loss: 16.8740\n",
            "Epoch 111/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 16.8939 - val_loss: 16.8692\n",
            "Epoch 112/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 16.8470 - val_loss: 16.7568\n",
            "Epoch 113/150\n",
            "142/142 [==============================] - 0s 115us/step - loss: 16.8448 - val_loss: 17.1543\n",
            "Epoch 114/150\n",
            "142/142 [==============================] - 0s 116us/step - loss: 16.8956 - val_loss: 16.9668\n",
            "Epoch 115/150\n",
            "142/142 [==============================] - 0s 102us/step - loss: 17.0988 - val_loss: 16.9632\n",
            "Epoch 116/150\n",
            "142/142 [==============================] - 0s 147us/step - loss: 16.8797 - val_loss: 16.9292\n",
            "Epoch 117/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 16.9966 - val_loss: 16.8771\n",
            "Epoch 118/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 16.8593 - val_loss: 16.8305\n",
            "Epoch 119/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 16.7623 - val_loss: 16.7434\n",
            "Epoch 120/150\n",
            "142/142 [==============================] - 0s 98us/step - loss: 16.7628 - val_loss: 16.8040\n",
            "Epoch 121/150\n",
            "142/142 [==============================] - 0s 101us/step - loss: 16.9574 - val_loss: 16.9378\n",
            "Epoch 122/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 16.8249 - val_loss: 16.8644\n",
            "Epoch 123/150\n",
            "142/142 [==============================] - 0s 113us/step - loss: 16.9451 - val_loss: 16.9321\n",
            "Epoch 124/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 16.7822 - val_loss: 16.9922\n",
            "Epoch 125/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 16.8973 - val_loss: 16.8319\n",
            "Epoch 126/150\n",
            "142/142 [==============================] - 0s 122us/step - loss: 16.8514 - val_loss: 16.9072\n",
            "Epoch 127/150\n",
            "142/142 [==============================] - 0s 103us/step - loss: 16.8797 - val_loss: 16.7487\n",
            "Epoch 128/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 16.7914 - val_loss: 17.0593\n",
            "Epoch 129/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 17.0531 - val_loss: 16.7147\n",
            "Epoch 130/150\n",
            "142/142 [==============================] - 0s 121us/step - loss: 16.8619 - val_loss: 16.6608\n",
            "Epoch 131/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 16.7672 - val_loss: 16.8429\n",
            "Epoch 132/150\n",
            "142/142 [==============================] - 0s 138us/step - loss: 16.7906 - val_loss: 16.6954\n",
            "Epoch 133/150\n",
            "142/142 [==============================] - 0s 112us/step - loss: 16.9938 - val_loss: 16.8636\n",
            "Epoch 134/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 16.7544 - val_loss: 16.7425\n",
            "Epoch 135/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 16.8622 - val_loss: 16.7849\n",
            "Epoch 136/150\n",
            "142/142 [==============================] - 0s 131us/step - loss: 16.6995 - val_loss: 16.8685\n",
            "Epoch 137/150\n",
            "142/142 [==============================] - 0s 103us/step - loss: 16.8810 - val_loss: 16.8042\n",
            "Epoch 138/150\n",
            "142/142 [==============================] - 0s 118us/step - loss: 16.6570 - val_loss: 16.8524\n",
            "Epoch 139/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 16.8838 - val_loss: 16.7693\n",
            "Epoch 140/150\n",
            "142/142 [==============================] - 0s 136us/step - loss: 17.0898 - val_loss: 16.9475\n",
            "Epoch 141/150\n",
            "142/142 [==============================] - 0s 107us/step - loss: 16.7282 - val_loss: 16.8078\n",
            "Epoch 142/150\n",
            "142/142 [==============================] - 0s 130us/step - loss: 16.7011 - val_loss: 16.8153\n",
            "Epoch 143/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 16.7497 - val_loss: 16.8681\n",
            "Epoch 144/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 16.8190 - val_loss: 16.6963\n",
            "Epoch 145/150\n",
            "142/142 [==============================] - 0s 112us/step - loss: 16.8771 - val_loss: 16.7754\n",
            "Epoch 146/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 16.7734 - val_loss: 16.8407\n",
            "Epoch 147/150\n",
            "142/142 [==============================] - 0s 124us/step - loss: 16.8275 - val_loss: 16.7578\n",
            "Epoch 148/150\n",
            "142/142 [==============================] - 0s 186us/step - loss: 16.9037 - val_loss: 16.7790\n",
            "Epoch 149/150\n",
            "142/142 [==============================] - 0s 152us/step - loss: 16.7240 - val_loss: 16.8026\n",
            "Epoch 150/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 16.7827 - val_loss: 16.7660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVjp817qf1VL",
        "colab_type": "text"
      },
      "source": [
        "## Generate data with VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiV2iQyVwSdN",
        "colab_type": "code",
        "outputId": "0aaaa62d-2232-46de-c219-db04d57edceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, original_dim))\n",
        "x_test_encoded = vae.encoder.predict(x_test)\n",
        "x_test_encoded = np.asarray(x_test_encoded)\n",
        "\n",
        "print(x_test_encoded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 142, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTfvSN_R2c76",
        "colab_type": "code",
        "outputId": "1943ad96-a636-4d72-f3d7-8eeba091fa68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "total_nums = 2\n",
        "results = []\n",
        "for i in range(x_test_encoded.shape[1]):\n",
        "    latent_gen = []\n",
        "    for _ in range(total_nums):\n",
        "        epsilon = np.random.normal(0., 1., x_test_encoded.shape[2])\n",
        "        latent_gen.extend([x_test_encoded[0, i, :] + np.exp(x_test_encoded[1, i, :]*0.5)*epsilon])\n",
        "    latent_gen = np.asarray(latent_gen)\n",
        "    results.append(vae.generate(latent_gen))\n",
        "    \n",
        "results = np.asarray(results)\n",
        "results = np.reshape(results, (-1, original_dim))\n",
        "print(results.shape)\n",
        "results = scaler.inverse_transform(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(284, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_Sfu9rgh8X",
        "colab_type": "text"
      },
      "source": [
        "## Handling generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdg-w-3eMquu",
        "colab_type": "code",
        "outputId": "58c18018-6082-4c3f-d69e-2c538f9f4508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(len(results[:, 1]))\n",
        "print(results[0, 0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "284\n",
            "12.172525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC6DiGV8MV9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {}\n",
        "names = list(df)\n",
        "for i, name in enumerate(names):\n",
        "    d[name] = results[:, i]\n",
        "df = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvdePB8gp1k",
        "colab_type": "text"
      },
      "source": [
        "## Re-categorical columns from generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_bbWWCsrHG",
        "colab_type": "code",
        "outputId": "b585b4c2-41c3-47a3-cffc-47af9223938e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "names = list(df)\n",
        "c_dict = {}\n",
        "for n in names:\n",
        "    if '_' in n:\n",
        "        index = n.index('_')\n",
        "        c_dict[n[:index]] = [c for c in names if n[:index+1] in c]\n",
        "values = []\n",
        "for key, items in c_dict.items():\n",
        "    dummies = df[items]\n",
        "    d_names = list(dummies)\n",
        "    c_dict = {}\n",
        "    for n in d_names:\n",
        "        c_dict[n] = n[n.index('_')+1:]\n",
        "    dummies.rename(columns=c_dict, \n",
        "                   inplace=True)\n",
        "    df[key] = dummies.idxmax(axis=1)\n",
        "    df.drop(items, axis=1, inplace=True)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          X1       X10       X11       X12  ...        X7        X8        X9  X0\n",
            "0  12.172525  0.059887  0.236538  1.170456  ...  0.035399  0.025448  0.171790   B\n",
            "1  14.687969  0.062780  0.468621  1.265368  ...  0.089523  0.057920  0.182368   B\n",
            "2  12.299572  0.062163  0.273778  1.184938  ...  0.050540  0.030058  0.174591   B\n",
            "3  12.190534  0.063317  0.266791  1.307913  ...  0.035017  0.022288  0.174360   B\n",
            "4  13.374092  0.061587  0.366202  1.241324  ...  0.076827  0.049000  0.176581   B\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  return super(DataFrame, self).rename(**kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ljK-ceANBih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "df.to_csv(path + '_vae.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SZr05ADu6LQP"
      },
      "source": [
        "# Dropout VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VSYitqlg1zw",
        "colab_type": "text"
      },
      "source": [
        "## Split train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d447533f-ff8c-4575-f848-86500057f7b4",
        "id": "oNqkHN9sxOm_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df = pd.read_csv(path + 'For_training.csv')\n",
        "train = np.copy(df.values)\n",
        "train.astype('float32')\n",
        "scaler = MinMaxScaler()\n",
        "print(np.amax(train[:, 2]))\n",
        "\n",
        "train = scaler.fit_transform(train)\n",
        "x_train, x_test = train_test_split(train, test_size=0.5,\n",
        "                                  random_state=LOCAL_SEED,\n",
        "                                  shuffle=True)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(np.amax(x_train))\n",
        "print(np.amax(x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.873\n",
            "(142, 32)\n",
            "(142, 32)\n",
            "1.0000000000000002\n",
            "1.0000000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4b91684e-f568-4399-d489-372ae53399b4",
        "id": "Kpetr9JKxOnK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "original_dim = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 32)\n",
            "(142, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX39keJ8g6sz",
        "colab_type": "text"
      },
      "source": [
        "## Define Dropout VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnKFnBX-6LQR",
        "colab": {}
      },
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.losses import categorical_crossentropy\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "class DropoutVAE:\n",
        "    def __init__(self, input_shape=(original_dim,), \n",
        "                 intermediate_dim=32, latent_dim=3, dropout=0.05, \n",
        "                 summary=False):\n",
        "        \n",
        "        self._build_model(input_shape,\n",
        "                         intermediate_dim, \n",
        "                          latent_dim, summary,\n",
        "                          dropout)\n",
        "    \n",
        "    def _build_model(self, input_shape, intermediate_dim, latent_dim,\n",
        "                    summary=False, dropout=0.05):\n",
        "        inputs = Input(shape=input_shape, name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(intermediate_dim, activation='relu')(x)\n",
        "        x = Dense(intermediate_dim//2, activation='relu')(x)\n",
        "        \n",
        "        z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "        \n",
        "        # We do not need this one\n",
        "#         z = Lambda(self.sampling, output_shape=(latent_dim,), \n",
        "#                    name='z')([z_mean, z_log_var])\n",
        "        \n",
        "        # We remove the z layer ( z layer is used in VAE but not here)\n",
        "        self.encoder = Model(inputs, [z_mean, z_log_var], \n",
        "                        name='encoder')\n",
        "        \n",
        "        latent_inputs = Input(shape=(latent_dim,), \n",
        "                              name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(intermediate_dim//2, activation='relu',\n",
        "                 kernel_regularizer=l2(1e-4),\n",
        "                 bias_regularizer=l2(1e-4))(x)\n",
        "        x = Dropout(dropout)(x)\n",
        "        x = Dense(intermediate_dim, activation='relu',\n",
        "                 kernel_regularizer=l2(1e-4),\n",
        "                 bias_regularizer=l2(1e-4))(x)\n",
        "        x = Dropout(dropout)(x)\n",
        "        outputs = Dense(original_dim, activation='sigmoid',\n",
        "                       kernel_regularizer=l2(1e-4),\n",
        "                       bias_regularizer=l2(1e-4))(x)\n",
        "\n",
        "        self.decoder = Model(latent_inputs, \n",
        "                             outputs, \n",
        "                             name='decoder')\n",
        "        \n",
        "        # Here we take the mean (not the z-layer) \n",
        "        outputs = self.decoder(self.encoder(inputs)[0])\n",
        "        self.vae = Model(inputs, outputs, \n",
        "                         name='vae_mlp')\n",
        "        \n",
        "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "        reconstruction_loss *= original_dim\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        \n",
        "        vae_loss = K.mean(reconstruction_loss + kl_loss)\t\n",
        "        \n",
        "        self.vae.add_loss(vae_loss)\n",
        "        self.vae.compile(optimizer='adam')\n",
        "        if summary: \n",
        "            print(self.vae.summary())\n",
        "        \n",
        "    # Remove this function\n",
        "#     def sampling(self, args):\n",
        "#         z_mean, z_log_var = args\n",
        "#         batch = K.shape(z_mean)[0]\n",
        "#         dim = K.int_shape(z_mean)[1]\n",
        "#         epsilon = K.random_normal(shape=(batch, dim))\n",
        "#         return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "        \n",
        "    def fit(self, x_train, x_test, epochs=100, batch_size=100,\n",
        "           verbose=1):\n",
        "        self.vae.fit(x_train, \n",
        "            shuffle=True,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=verbose,\n",
        "            validation_data=(x_test, None))\n",
        "    \n",
        "    def encoder_predict(self, x_test, batch_size=100):\n",
        "        return self.encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    \n",
        "    def generate(self, latent_val, batch_size=100):\n",
        "        return self.decoder.predict(latent_val)\n",
        "    \n",
        "    def predict(self, x_test, batch_size=1, nums=1000):\n",
        "        Yt_hat = []\n",
        "        for _ in range(nums):\n",
        "            Yt_hat.extend(self.vae.predict(x_test))\n",
        "                          \n",
        "        return np.asarray(Yt_hat)\n",
        "                          \n",
        "    def mean_predict(self, x_test, batch_size=1, nums=1000):\n",
        "        predict_stochastic = K.function([self.decoder.layers[0].input,\n",
        "                                K.learning_phase()],\n",
        "                                [self.decoder.get_output_at(0)])\n",
        "        latents = self.encoder.predict(x_test)[0]\n",
        "        Yt_hat = []\n",
        "        for _ in range(nums):\n",
        "            Yt_hat.append(predict_stochastic([latents, 1])) \n",
        "        return np.asarray(Yt_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TsARTKz36LQZ"
      },
      "source": [
        "## Train and evaluate Dropout VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkdmVggK6LQp",
        "outputId": "ddda28ed-f798-4e67-bf1d-1bb849d8a192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5715
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "vae = DropoutVAE(intermediate_dim=intermediate_dim,\n",
        "                 dropout=0.2, latent_dim=latent_dim,\n",
        "                 summary=True)\n",
        "vae.fit(x_train, x_test, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 16:12:59.132649 140515461060480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0615 16:12:59.145390 140515461060480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0615 16:12:59.147334 140515461060480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0615 16:12:59.219371 140515461060480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0615 16:12:59.229673 140515461060480 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0615 16:12:59.339346 140515461060480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0615 16:12:59.345251 140515461060480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0615 16:12:59.359994 140515461060480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 16), (None, 16)]  156448    \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 32)                152352    \n",
            "=================================================================\n",
            "Total params: 308,800\n",
            "Trainable params: 308,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 142 samples, validate on 142 samples\n",
            "Epoch 1/150\n",
            "142/142 [==============================] - 4s 29ms/step - loss: 22.3021 - val_loss: 22.0906\n",
            "Epoch 2/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 22.0583 - val_loss: 21.7457\n",
            "Epoch 3/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 21.7037 - val_loss: 21.1129\n",
            "Epoch 4/150\n",
            "142/142 [==============================] - 0s 115us/step - loss: 21.0384 - val_loss: 20.1952\n",
            "Epoch 5/150\n",
            "142/142 [==============================] - 0s 122us/step - loss: 20.1843 - val_loss: 19.2163\n",
            "Epoch 6/150\n",
            "142/142 [==============================] - 0s 116us/step - loss: 19.1627 - val_loss: 18.4533\n",
            "Epoch 7/150\n",
            "142/142 [==============================] - 0s 111us/step - loss: 18.3419 - val_loss: 17.9619\n",
            "Epoch 8/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 17.8725 - val_loss: 17.4667\n",
            "Epoch 9/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 17.4377 - val_loss: 17.0547\n",
            "Epoch 10/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 17.1469 - val_loss: 16.8127\n",
            "Epoch 11/150\n",
            "142/142 [==============================] - 0s 99us/step - loss: 16.9156 - val_loss: 16.6443\n",
            "Epoch 12/150\n",
            "142/142 [==============================] - 0s 146us/step - loss: 16.7175 - val_loss: 16.5172\n",
            "Epoch 13/150\n",
            "142/142 [==============================] - 0s 145us/step - loss: 16.5853 - val_loss: 16.3480\n",
            "Epoch 14/150\n",
            "142/142 [==============================] - 0s 138us/step - loss: 16.4266 - val_loss: 16.2166\n",
            "Epoch 15/150\n",
            "142/142 [==============================] - 0s 138us/step - loss: 16.3003 - val_loss: 16.0647\n",
            "Epoch 16/150\n",
            "142/142 [==============================] - 0s 154us/step - loss: 16.1687 - val_loss: 15.9819\n",
            "Epoch 17/150\n",
            "142/142 [==============================] - 0s 138us/step - loss: 16.0767 - val_loss: 15.9207\n",
            "Epoch 18/150\n",
            "142/142 [==============================] - 0s 125us/step - loss: 15.9965 - val_loss: 15.8458\n",
            "Epoch 19/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 15.9286 - val_loss: 15.7944\n",
            "Epoch 20/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.8884 - val_loss: 15.7415\n",
            "Epoch 21/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 15.8442 - val_loss: 15.7001\n",
            "Epoch 22/150\n",
            "142/142 [==============================] - 0s 129us/step - loss: 15.7883 - val_loss: 15.6610\n",
            "Epoch 23/150\n",
            "142/142 [==============================] - 0s 137us/step - loss: 15.7639 - val_loss: 15.6218\n",
            "Epoch 24/150\n",
            "142/142 [==============================] - 0s 126us/step - loss: 15.7243 - val_loss: 15.5853\n",
            "Epoch 25/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 15.6844 - val_loss: 15.5553\n",
            "Epoch 26/150\n",
            "142/142 [==============================] - 0s 121us/step - loss: 15.6439 - val_loss: 15.5261\n",
            "Epoch 27/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.6139 - val_loss: 15.5041\n",
            "Epoch 28/150\n",
            "142/142 [==============================] - 0s 125us/step - loss: 15.6060 - val_loss: 15.4742\n",
            "Epoch 29/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 15.5679 - val_loss: 15.4574\n",
            "Epoch 30/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.5536 - val_loss: 15.4120\n",
            "Epoch 31/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 15.5205 - val_loss: 15.3826\n",
            "Epoch 32/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 15.4901 - val_loss: 15.3557\n",
            "Epoch 33/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.4614 - val_loss: 15.3180\n",
            "Epoch 34/150\n",
            "142/142 [==============================] - 0s 139us/step - loss: 15.4168 - val_loss: 15.2756\n",
            "Epoch 35/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 15.3876 - val_loss: 15.2421\n",
            "Epoch 36/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 15.3604 - val_loss: 15.2090\n",
            "Epoch 37/150\n",
            "142/142 [==============================] - 0s 113us/step - loss: 15.3513 - val_loss: 15.1782\n",
            "Epoch 38/150\n",
            "142/142 [==============================] - 0s 110us/step - loss: 15.2980 - val_loss: 15.1631\n",
            "Epoch 39/150\n",
            "142/142 [==============================] - 0s 161us/step - loss: 15.2822 - val_loss: 15.1332\n",
            "Epoch 40/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 15.2630 - val_loss: 15.1318\n",
            "Epoch 41/150\n",
            "142/142 [==============================] - 0s 135us/step - loss: 15.2760 - val_loss: 15.1221\n",
            "Epoch 42/150\n",
            "142/142 [==============================] - 0s 137us/step - loss: 15.2514 - val_loss: 15.1343\n",
            "Epoch 43/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 15.2444 - val_loss: 15.0975\n",
            "Epoch 44/150\n",
            "142/142 [==============================] - 0s 137us/step - loss: 15.2240 - val_loss: 15.1069\n",
            "Epoch 45/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.2105 - val_loss: 15.0944\n",
            "Epoch 46/150\n",
            "142/142 [==============================] - 0s 121us/step - loss: 15.1996 - val_loss: 15.0679\n",
            "Epoch 47/150\n",
            "142/142 [==============================] - 0s 117us/step - loss: 15.1693 - val_loss: 15.0339\n",
            "Epoch 48/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.1474 - val_loss: 15.0183\n",
            "Epoch 49/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 15.1214 - val_loss: 15.0070\n",
            "Epoch 50/150\n",
            "142/142 [==============================] - 0s 130us/step - loss: 15.1063 - val_loss: 14.9916\n",
            "Epoch 51/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 15.0924 - val_loss: 14.9840\n",
            "Epoch 52/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 15.0818 - val_loss: 14.9707\n",
            "Epoch 53/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.0680 - val_loss: 14.9631\n",
            "Epoch 54/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 15.0489 - val_loss: 14.9561\n",
            "Epoch 55/150\n",
            "142/142 [==============================] - 0s 132us/step - loss: 15.0417 - val_loss: 14.9344\n",
            "Epoch 56/150\n",
            "142/142 [==============================] - 0s 118us/step - loss: 15.0387 - val_loss: 14.9277\n",
            "Epoch 57/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 15.0162 - val_loss: 14.9430\n",
            "Epoch 58/150\n",
            "142/142 [==============================] - 0s 135us/step - loss: 15.0280 - val_loss: 14.9192\n",
            "Epoch 59/150\n",
            "142/142 [==============================] - 0s 125us/step - loss: 15.0283 - val_loss: 14.9096\n",
            "Epoch 60/150\n",
            "142/142 [==============================] - 0s 139us/step - loss: 15.0113 - val_loss: 14.9090\n",
            "Epoch 61/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 15.0070 - val_loss: 14.9179\n",
            "Epoch 62/150\n",
            "142/142 [==============================] - 0s 125us/step - loss: 15.0111 - val_loss: 14.8901\n",
            "Epoch 63/150\n",
            "142/142 [==============================] - 0s 116us/step - loss: 14.9755 - val_loss: 14.9212\n",
            "Epoch 64/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 14.9827 - val_loss: 14.8977\n",
            "Epoch 65/150\n",
            "142/142 [==============================] - 0s 124us/step - loss: 14.9765 - val_loss: 14.9059\n",
            "Epoch 66/150\n",
            "142/142 [==============================] - 0s 106us/step - loss: 14.9636 - val_loss: 14.8874\n",
            "Epoch 67/150\n",
            "142/142 [==============================] - 0s 109us/step - loss: 14.9681 - val_loss: 14.8718\n",
            "Epoch 68/150\n",
            "142/142 [==============================] - 0s 108us/step - loss: 14.9405 - val_loss: 14.8829\n",
            "Epoch 69/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 14.9480 - val_loss: 14.8582\n",
            "Epoch 70/150\n",
            "142/142 [==============================] - 0s 166us/step - loss: 14.9362 - val_loss: 14.8638\n",
            "Epoch 71/150\n",
            "142/142 [==============================] - 0s 124us/step - loss: 14.9476 - val_loss: 14.8618\n",
            "Epoch 72/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 14.9221 - val_loss: 14.8568\n",
            "Epoch 73/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 14.9217 - val_loss: 14.8512\n",
            "Epoch 74/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 14.9220 - val_loss: 14.8572\n",
            "Epoch 75/150\n",
            "142/142 [==============================] - 0s 123us/step - loss: 14.9242 - val_loss: 14.8540\n",
            "Epoch 76/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 14.9146 - val_loss: 14.8461\n",
            "Epoch 77/150\n",
            "142/142 [==============================] - 0s 114us/step - loss: 14.9309 - val_loss: 14.8366\n",
            "Epoch 78/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 14.9078 - val_loss: 14.8309\n",
            "Epoch 79/150\n",
            "142/142 [==============================] - 0s 122us/step - loss: 14.9083 - val_loss: 14.8596\n",
            "Epoch 80/150\n",
            "142/142 [==============================] - 0s 119us/step - loss: 14.9114 - val_loss: 14.8231\n",
            "Epoch 81/150\n",
            "142/142 [==============================] - 0s 137us/step - loss: 14.9053 - val_loss: 14.8289\n",
            "Epoch 82/150\n",
            "142/142 [==============================] - 0s 134us/step - loss: 14.8999 - val_loss: 14.8183\n",
            "Epoch 83/150\n",
            "142/142 [==============================] - 0s 160us/step - loss: 14.8981 - val_loss: 14.8304\n",
            "Epoch 84/150\n",
            "142/142 [==============================] - 0s 135us/step - loss: 14.8890 - val_loss: 14.8315\n",
            "Epoch 85/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 14.8917 - val_loss: 14.8503\n",
            "Epoch 86/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 14.9050 - val_loss: 14.8072\n",
            "Epoch 87/150\n",
            "142/142 [==============================] - 0s 122us/step - loss: 14.8838 - val_loss: 14.8436\n",
            "Epoch 88/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 14.8910 - val_loss: 14.8418\n",
            "Epoch 89/150\n",
            "142/142 [==============================] - 0s 136us/step - loss: 14.9248 - val_loss: 14.8455\n",
            "Epoch 90/150\n",
            "142/142 [==============================] - 0s 139us/step - loss: 14.8915 - val_loss: 14.7814\n",
            "Epoch 91/150\n",
            "142/142 [==============================] - 0s 161us/step - loss: 14.8641 - val_loss: 14.7899\n",
            "Epoch 92/150\n",
            "142/142 [==============================] - 0s 172us/step - loss: 14.8803 - val_loss: 14.8092\n",
            "Epoch 93/150\n",
            "142/142 [==============================] - 0s 129us/step - loss: 14.8704 - val_loss: 14.7740\n",
            "Epoch 94/150\n",
            "142/142 [==============================] - 0s 135us/step - loss: 14.8575 - val_loss: 14.7942\n",
            "Epoch 95/150\n",
            "142/142 [==============================] - 0s 147us/step - loss: 14.8565 - val_loss: 14.7741\n",
            "Epoch 96/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 14.8394 - val_loss: 14.7667\n",
            "Epoch 97/150\n",
            "142/142 [==============================] - 0s 145us/step - loss: 14.8520 - val_loss: 14.7652\n",
            "Epoch 98/150\n",
            "142/142 [==============================] - 0s 148us/step - loss: 14.8407 - val_loss: 14.7500\n",
            "Epoch 99/150\n",
            "142/142 [==============================] - 0s 145us/step - loss: 14.8341 - val_loss: 14.7678\n",
            "Epoch 100/150\n",
            "142/142 [==============================] - 0s 156us/step - loss: 14.8388 - val_loss: 14.7507\n",
            "Epoch 101/150\n",
            "142/142 [==============================] - 0s 142us/step - loss: 14.8376 - val_loss: 14.7557\n",
            "Epoch 102/150\n",
            "142/142 [==============================] - 0s 148us/step - loss: 14.8398 - val_loss: 14.7788\n",
            "Epoch 103/150\n",
            "142/142 [==============================] - 0s 137us/step - loss: 14.8409 - val_loss: 14.7458\n",
            "Epoch 104/150\n",
            "142/142 [==============================] - 0s 134us/step - loss: 14.8397 - val_loss: 14.7712\n",
            "Epoch 105/150\n",
            "142/142 [==============================] - 0s 122us/step - loss: 14.8327 - val_loss: 14.7365\n",
            "Epoch 106/150\n",
            "142/142 [==============================] - 0s 146us/step - loss: 14.8214 - val_loss: 14.7505\n",
            "Epoch 107/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 14.8156 - val_loss: 14.7390\n",
            "Epoch 108/150\n",
            "142/142 [==============================] - 0s 136us/step - loss: 14.8139 - val_loss: 14.7304\n",
            "Epoch 109/150\n",
            "142/142 [==============================] - 0s 158us/step - loss: 14.8015 - val_loss: 14.7236\n",
            "Epoch 110/150\n",
            "142/142 [==============================] - 0s 138us/step - loss: 14.8055 - val_loss: 14.7106\n",
            "Epoch 111/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 14.7901 - val_loss: 14.7269\n",
            "Epoch 112/150\n",
            "142/142 [==============================] - 0s 126us/step - loss: 14.7920 - val_loss: 14.7197\n",
            "Epoch 113/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 14.7988 - val_loss: 14.7394\n",
            "Epoch 114/150\n",
            "142/142 [==============================] - 0s 135us/step - loss: 14.8026 - val_loss: 14.7028\n",
            "Epoch 115/150\n",
            "142/142 [==============================] - 0s 127us/step - loss: 14.7858 - val_loss: 14.7078\n",
            "Epoch 116/150\n",
            "142/142 [==============================] - 0s 120us/step - loss: 14.7878 - val_loss: 14.7124\n",
            "Epoch 117/150\n",
            "142/142 [==============================] - 0s 131us/step - loss: 14.7898 - val_loss: 14.7071\n",
            "Epoch 118/150\n",
            "142/142 [==============================] - 0s 137us/step - loss: 14.7700 - val_loss: 14.6981\n",
            "Epoch 119/150\n",
            "142/142 [==============================] - 0s 174us/step - loss: 14.7677 - val_loss: 14.6963\n",
            "Epoch 120/150\n",
            "142/142 [==============================] - 0s 189us/step - loss: 14.7629 - val_loss: 14.6971\n",
            "Epoch 121/150\n",
            "142/142 [==============================] - 0s 154us/step - loss: 14.7662 - val_loss: 14.6874\n",
            "Epoch 122/150\n",
            "142/142 [==============================] - 0s 143us/step - loss: 14.7587 - val_loss: 14.6961\n",
            "Epoch 123/150\n",
            "142/142 [==============================] - 0s 157us/step - loss: 14.7619 - val_loss: 14.6824\n",
            "Epoch 124/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 14.7679 - val_loss: 14.6941\n",
            "Epoch 125/150\n",
            "142/142 [==============================] - 0s 131us/step - loss: 14.7567 - val_loss: 14.6849\n",
            "Epoch 126/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 14.7614 - val_loss: 14.6887\n",
            "Epoch 127/150\n",
            "142/142 [==============================] - 0s 126us/step - loss: 14.7583 - val_loss: 14.6716\n",
            "Epoch 128/150\n",
            "142/142 [==============================] - 0s 154us/step - loss: 14.7591 - val_loss: 14.6881\n",
            "Epoch 129/150\n",
            "142/142 [==============================] - 0s 134us/step - loss: 14.7646 - val_loss: 14.6609\n",
            "Epoch 130/150\n",
            "142/142 [==============================] - 0s 126us/step - loss: 14.7413 - val_loss: 14.6624\n",
            "Epoch 131/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 14.7402 - val_loss: 14.6620\n",
            "Epoch 132/150\n",
            "142/142 [==============================] - 0s 136us/step - loss: 14.7336 - val_loss: 14.6562\n",
            "Epoch 133/150\n",
            "142/142 [==============================] - 0s 134us/step - loss: 14.7498 - val_loss: 14.6663\n",
            "Epoch 134/150\n",
            "142/142 [==============================] - 0s 165us/step - loss: 14.7433 - val_loss: 14.6452\n",
            "Epoch 135/150\n",
            "142/142 [==============================] - 0s 140us/step - loss: 14.7425 - val_loss: 14.6490\n",
            "Epoch 136/150\n",
            "142/142 [==============================] - 0s 141us/step - loss: 14.7356 - val_loss: 14.6472\n",
            "Epoch 137/150\n",
            "142/142 [==============================] - 0s 129us/step - loss: 14.7357 - val_loss: 14.6432\n",
            "Epoch 138/150\n",
            "142/142 [==============================] - 0s 150us/step - loss: 14.7287 - val_loss: 14.6474\n",
            "Epoch 139/150\n",
            "142/142 [==============================] - 0s 138us/step - loss: 14.7295 - val_loss: 14.6565\n",
            "Epoch 140/150\n",
            "142/142 [==============================] - 0s 154us/step - loss: 14.7281 - val_loss: 14.6416\n",
            "Epoch 141/150\n",
            "142/142 [==============================] - 0s 151us/step - loss: 14.7322 - val_loss: 14.6377\n",
            "Epoch 142/150\n",
            "142/142 [==============================] - 0s 139us/step - loss: 14.7319 - val_loss: 14.6463\n",
            "Epoch 143/150\n",
            "142/142 [==============================] - 0s 148us/step - loss: 14.7258 - val_loss: 14.6349\n",
            "Epoch 144/150\n",
            "142/142 [==============================] - 0s 145us/step - loss: 14.7226 - val_loss: 14.6411\n",
            "Epoch 145/150\n",
            "142/142 [==============================] - 0s 128us/step - loss: 14.7324 - val_loss: 14.6493\n",
            "Epoch 146/150\n",
            "142/142 [==============================] - 0s 133us/step - loss: 14.7332 - val_loss: 14.6419\n",
            "Epoch 147/150\n",
            "142/142 [==============================] - 0s 157us/step - loss: 14.7251 - val_loss: 14.6286\n",
            "Epoch 148/150\n",
            "142/142 [==============================] - 0s 147us/step - loss: 14.7288 - val_loss: 14.6316\n",
            "Epoch 149/150\n",
            "142/142 [==============================] - 0s 148us/step - loss: 14.7341 - val_loss: 14.6387\n",
            "Epoch 150/150\n",
            "142/142 [==============================] - 0s 136us/step - loss: 14.7213 - val_loss: 14.6430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0vCRcnhA6l",
        "colab_type": "text"
      },
      "source": [
        "## Generate data with Dropout VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2HndA3MG6LQs",
        "outputId": "a04d3d47-7205-4a4c-b656-b79e75b25a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, original_dim))\n",
        "print(x_test.shape)\n",
        "print(x_test[0].reshape(-1, original_dim).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 32)\n",
            "(1, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgO9JKSqWsW",
        "colab_type": "code",
        "outputId": "c49a1f89-5af4-4d9b-8769-00e4ae2bb2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "total_nums = 2\n",
        "results = []\n",
        "\n",
        "x_test_encoded = vae.mean_predict(x_test, nums=total_nums)\n",
        "print(x_test_encoded.shape)\n",
        "\n",
        "results = x_test_encoded\n",
        "results = results.reshape(total_nums*results.shape[2], original_dim)\n",
        "results = scaler.inverse_transform(results)\n",
        "print(results.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1, 142, 32)\n",
            "(284, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POfJxgHamlmx",
        "colab_type": "text"
      },
      "source": [
        "## Handling Generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YIlGSzekAYlO",
        "colab": {}
      },
      "source": [
        "d = {}\n",
        "names = list(df)\n",
        "for i, name in enumerate(names):\n",
        "    d[name] = results[:, i]\n",
        "df = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkAZ6kCchNII",
        "colab_type": "text"
      },
      "source": [
        "## Re-categoricalize data from Generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "shurAQCSAYlV",
        "outputId": "8e3739ca-5fa3-42aa-dedb-4d4111519408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "names = list(df)\n",
        "c_dict = {}\n",
        "for n in names:\n",
        "    if '_' in n:\n",
        "        index = n.index('_')\n",
        "        c_dict[n[:index]] = [c for c in names if n[:index+1] in c]\n",
        "values = []\n",
        "for key, items in c_dict.items():\n",
        "    dummies = df[items]\n",
        "    d_names = list(dummies)\n",
        "    c_dict = {}\n",
        "    for n in d_names:\n",
        "        c_dict[n] = n[n.index('_')+1:]\n",
        "    dummies.rename(columns=c_dict, \n",
        "                   inplace=True)\n",
        "    df[key] = dummies.idxmax(axis=1)\n",
        "    df.drop(items, axis=1, inplace=True)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          X1       X10       X11       X12  ...        X7        X8        X9  X0\n",
            "0  16.453360  0.057111  0.389558  1.073235  ...  0.052454  0.033973  0.168655   B\n",
            "1  12.289575  0.061267  0.193680  0.582741  ...  0.056750  0.029594  0.178056   B\n",
            "2  10.439020  0.066813  0.346121  1.787749  ...  0.031502  0.016874  0.165296   B\n",
            "3  12.747104  0.059958  0.361127  1.825076  ...  0.058108  0.028500  0.150278   B\n",
            "4  20.998669  0.056213  0.634688  0.740990  ...  0.172398  0.103170  0.168748   M\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  return super(DataFrame, self).rename(**kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8dLIoyVW_Kn",
        "colab": {}
      },
      "source": [
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "df.to_csv(path + '_dropout.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL6N8v2sYTNm",
        "colab_type": "text"
      },
      "source": [
        "# Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVhmliYYzwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(path + '_For_Test.csv',\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "df_mc = pd.read_csv(path + '_dropout.csv',\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "df_vae = pd.read_csv(path + '_vae.csv',\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "names = list(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSbyWbb0ZjAJ",
        "colab_type": "code",
        "outputId": "9f25241f-686f-4545-a679-7b4290f27ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "colnums = len(df.columns)\n",
        "for i in df.columns:\n",
        "    try:\n",
        "        if df[i].dtype.name == 'object':\n",
        "            df[i] = df[i].astype('category')\n",
        "    except:\n",
        "        continue\n",
        "cat_columns = df.select_dtypes(['category']).columns\n",
        "print(cat_columns)\n",
        "for col in cat_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].values)\n",
        "    df_mc[col] = le.transform(df_mc[col].values)\n",
        "    df_vae[col] = le.transform(df_vae[col].values)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['X0'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-scTKTHZ9Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "df_mc = df_mc.reindex(sorted(df_mc.columns), axis=1)\n",
        "df_vae = df_vae.reindex(sorted(df_vae.columns), axis=1)\n",
        "df.to_csv(path + '_For_Test_encoded.csv')\n",
        "df_mc.to_csv(path + '_dropout_encoded.csv')\n",
        "df_vae.to_csv(path + '_vae_encoded.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}